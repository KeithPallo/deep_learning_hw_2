{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning from scratch: homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the exercises listed below in this Jupyter notebook - leaving all of your code in Python cells in the notebook itself.  Feel free to add any necessary cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Exercise 1. </span>   Perform two-class classification on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code up the two-class logistic regression / softmax cost function, using gradient descent to minimize.  You should use the two class toy dataset in the file called *3d_classification_data_v2.csv*.\n",
    "\n",
    "Create a plot with two panels that shows the number of misclassifications at each gradient descent step (in the left panel), and one that compares the cost function at each gradient descent step (in the right panel).  You won't get perfect separation - but you should be able to separate most of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_1 = np.loadtxt(\"3d_classification_data_v2.csv\",delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = data_1[:,:-1]\n",
    "y_1 = data_1[:,-1]\n",
    "np.place(y_1,y_1<0,0)\n",
    "y_1_hot = np.eye(2)[y_1.astype(int)]\n",
    "w_1 = np.random.randn(np.shape(x_1)[1]+1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1_hot.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    \n",
    "    #(np.exp((z))) / np.sum(np.exp((z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W):\n",
    "    return (W[0] + np.dot(X,W[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predict(z):\n",
    "    return z.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification(y_true,y_guess):\n",
    "    return (np.sum(y_true == y_guess))/y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(W,X,y_hot_true):\n",
    "    output = model(X,W)\n",
    "    softmax_values = softmax(output)\n",
    "    return (np.mean((-np.sum(np.log(softmax_values)*y_hot_true,axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_predict(W,X,Y):\n",
    "    raw = model(X,W) \n",
    "    soft = softmax(raw)\n",
    "    print(soft)\n",
    "    classes = class_predict(soft)\n",
    "    print(classes)\n",
    "    return misclassification(Y,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7383567698911"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(w_1,x_1,y_1_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent function - inputs: g (input function), alpha (steplength parameter), max_its (maximum number of iterations), w (initialization)\n",
    "def gradient_descent(g,alpha,max_its,w,x,y_hot):\n",
    "    # compute the gradient of our input function - note this is a function too!\n",
    "    \n",
    "    gradient = grad(g)\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    best_w = w             # weight we return, should be the one providing lowest evaluation\n",
    "    best_eval = g(w,x,y_hot)       # lowest evaluation yet\n",
    "    for k in range(max_its):\n",
    "        # evaluate the gradient\n",
    "        grad_eval = gradient(w,x,y_hot)\n",
    "        #print(grad_eval)\n",
    "\n",
    "        # take gradient descent step\n",
    "        w = w - alpha*grad_eval\n",
    "        \n",
    "        # return only the weight providing the lowest evaluation\n",
    "        test_eval = g(w,x,y_hot)\n",
    "        if test_eval < best_eval:\n",
    "            best_eval = test_eval\n",
    "            best_w = w\n",
    "            \n",
    "    return best_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.80406362 -0.68571936]\n",
      " [ 1.07496816  0.06015715]\n",
      " [ 1.02560338 -0.45653016]]\n"
     ]
    }
   ],
   "source": [
    "best = gradient_descent(g=cost,alpha = .0015,max_its = 10000,w = w_1, x=x_1, y_hot=y_1_hot)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9497291  -1.54005387]\n",
      " [ 0.43414388  0.70098143]\n",
      " [ 0.3253405   0.24373272]]\n"
     ]
    }
   ],
   "source": [
    "print(w_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51222918208625"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(best,x_1,y_1_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39636358 0.60363642]\n",
      " [0.40264491 0.59735509]\n",
      " [0.39169457 0.60830543]\n",
      " [0.45098398 0.54901602]\n",
      " [0.48025004 0.51974996]\n",
      " [0.33709277 0.66290723]\n",
      " [0.41421122 0.58578878]\n",
      " [0.44800038 0.55199962]\n",
      " [0.46487683 0.53512317]\n",
      " [0.37517735 0.62482265]\n",
      " [0.28754567 0.71245433]\n",
      " [0.37886017 0.62113983]\n",
      " [0.44364725 0.55635275]\n",
      " [0.48668493 0.51331507]\n",
      " [0.5006028  0.4993972 ]\n",
      " [0.40329835 0.59670165]\n",
      " [0.42616197 0.57383803]\n",
      " [0.3905395  0.6094605 ]\n",
      " [0.48581514 0.51418486]\n",
      " [0.44948252 0.55051748]\n",
      " [0.47301395 0.52698605]\n",
      " [0.48708482 0.51291518]\n",
      " [0.51450436 0.48549564]\n",
      " [0.3679408  0.6320592 ]\n",
      " [0.53594462 0.46405538]\n",
      " [0.55379418 0.44620582]\n",
      " [0.47121894 0.52878106]\n",
      " [0.36813565 0.63186435]\n",
      " [0.3322822  0.6677178 ]\n",
      " [0.46293206 0.53706794]\n",
      " [0.40761617 0.59238383]\n",
      " [0.34130147 0.65869853]\n",
      " [0.47887173 0.52112827]\n",
      " [0.55428095 0.44571905]\n",
      " [0.41409577 0.58590423]\n",
      " [0.38381911 0.61618089]\n",
      " [0.56269944 0.43730056]\n",
      " [0.48846383 0.51153617]\n",
      " [0.47845347 0.52154653]\n",
      " [0.44032364 0.55967636]\n",
      " [0.4584231  0.5415769 ]\n",
      " [0.5944159  0.4055841 ]\n",
      " [0.35111547 0.64888453]\n",
      " [0.55577524 0.44422476]\n",
      " [0.28386608 0.71613392]\n",
      " [0.3447447  0.6552553 ]\n",
      " [0.49817376 0.50182624]\n",
      " [0.48589792 0.51410208]\n",
      " [0.37220946 0.62779054]\n",
      " [0.43975619 0.56024381]\n",
      " [0.71611806 0.28388194]\n",
      " [0.55192405 0.44807595]\n",
      " [0.67393209 0.32606791]\n",
      " [0.68537093 0.31462907]\n",
      " [0.57201644 0.42798356]\n",
      " [0.73787651 0.26212349]\n",
      " [0.6950673  0.3049327 ]\n",
      " [0.67285017 0.32714983]\n",
      " [0.63255373 0.36744627]\n",
      " [0.67768413 0.32231587]\n",
      " [0.54648265 0.45351735]\n",
      " [0.68639619 0.31360381]\n",
      " [0.61625734 0.38374266]\n",
      " [0.65260614 0.34739386]\n",
      " [0.62926547 0.37073453]\n",
      " [0.67402936 0.32597064]\n",
      " [0.59904051 0.40095949]\n",
      " [0.55377331 0.44622669]\n",
      " [0.54719653 0.45280347]\n",
      " [0.69664897 0.30335103]\n",
      " [0.73789989 0.26210011]\n",
      " [0.61910627 0.38089373]\n",
      " [0.71509242 0.28490758]\n",
      " [0.63163346 0.36836654]\n",
      " [0.59374558 0.40625442]\n",
      " [0.69876142 0.30123858]\n",
      " [0.66491902 0.33508098]\n",
      " [0.66901921 0.33098079]\n",
      " [0.62610478 0.37389522]\n",
      " [0.63196868 0.36803132]\n",
      " [0.63557284 0.36442716]\n",
      " [0.64816332 0.35183668]\n",
      " [0.66794869 0.33205131]\n",
      " [0.60219497 0.39780503]\n",
      " [0.50864861 0.49135139]\n",
      " [0.69054284 0.30945716]\n",
      " [0.57737226 0.42262774]\n",
      " [0.70312974 0.29687026]\n",
      " [0.65008829 0.34991171]\n",
      " [0.69283403 0.30716597]\n",
      " [0.73181322 0.26818678]\n",
      " [0.64037443 0.35962557]\n",
      " [0.63313991 0.36686009]\n",
      " [0.68331185 0.31668815]\n",
      " [0.67444608 0.32555392]\n",
      " [0.66883575 0.33116425]\n",
      " [0.66551308 0.33448692]\n",
      " [0.54817859 0.45182141]\n",
      " [0.73525866 0.26474134]\n",
      " [0.5824192  0.4175808 ]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 0 1 1 0\n",
      " 1 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_predict(best,x_1,y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft = softmax(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1825525843986018"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(soft,y_1_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = to_classlabel(soft)\n",
    "first.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.place(y_1,y_1<0,0)\n",
    "y_1.shape\n",
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification(y_1,first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Exercise 2. </span>   Perform two-class classification on a breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the softmax cost function to classify healthy from cancerous tissue using the dataset located in breast_cancer_dataset.csv (included in this homework folder).  You can examine the description of this dataset [here](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). \n",
    "\n",
    "There are $N = 8$ input dimensions to the input of this dataset (these are the first $N = 8$ columns of the breast_cancer_dataset.csv, the last column are the associated labels).  Fit using gradient descent using a maximum of 5,000 iterations.  You should be able to reach a point on the surface where you misclassify less than 30 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Python is a great prototyping language but [it is slow](http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/), particular when evaluating explicit for loops.  If you are having speed issues try re-writing the softmax cost function using as few explicit for-loops as possible (you can indeed write the entire summation in a single line of Python code, for-loop free)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Exercise 3. </span>   Code up One-versus-All multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the toy $C = 3$ class dataset located in *3class_data.csv* - code up One-Versus-All classification, using this toy dataset to test out your code.  You should be able to learn a model that perfectly separates this data - as shown in class.  You may use your softmax cost / gradient descent code here for each of the two-class subproblems! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#a50e3e;\">Exercise 4. </span>   A nonlinear two-class dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propose a nonlinear feature transformation and integrate it into your two-class classification scheme in order to adequately classify the dataset shown below - located in the file *bricks.csv'.  With the right transformation you should be able to classify this quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "  <img src= 'brick_pick.png' width=\"40%\" height=\"40%\" alt=\"\"/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
